{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dcca71b-b72f-4699-b956-1e25929aa311",
   "metadata": {},
   "source": [
    "# Optimizer Step: Integrate Localization and Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e0feb0-c6bc-4079-9946-32bf0e96f301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import transformer_lens\n",
    "import torch, gc, itertools, functools, tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/')\n",
    "from paraMem.utils import modelHandlers, dataLoaders, gradient, evaluation, localizing, intervening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67623921-6d0d-498e-a112-4a2ccccad640",
   "metadata": {},
   "source": [
    "## Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b84de32-0f84-4ac8-b393-e45add133d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt-neo-125M into HookedTransformer\n",
      "setting no_grad on ['embed', 'pos_embed', 'unembed', 'b_in', 'b_out', 'b_K', 'b_Q', 'b_V', 'b_O']\n"
     ]
    }
   ],
   "source": [
    "model = modelHandlers.load_model(model_type=\"gpt-neo-125M\", DEVICE=\"cuda\", lr=0.0, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee438f85-dd13-4d26-b9ac-9078b111f17d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b51b9c7-afac-4957-8db9-44e200599e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mem and non-mem set\n",
    "#(mem_prompts, mem_counts),(non_mem_prompts,non_mem_counts) = dataLoaders.load_pile_splits(\"acc/gpt-neo-125M\", as_torch=True)\n",
    "#train_dl, test_dl = dataLoaders.train_test_batching(mem_prompts, non_mem_prompts, mem_batch=5, non_mem_batch=5, test_frac=0.0, set_twice=None)\n",
    "#c_toks_NI, k_toks_NI = next(iter(train_dl))\n",
    "\n",
    "## load perturbed mem set and original mem set\n",
    "mem_prompts, non_mem_prompts = dataLoaders.load_perturbed_mem(file_path=\"acc/gpt-neo-125M\")\n",
    "train_dl = torch.utils.data.DataLoader(list(zip(mem_prompts, non_mem_prompts)), batch_size=3, shuffle=False)\n",
    "c_toks_NI, k_toks_NI = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49a654f-75c2-4497-bae0-93efcb41851f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dc09822ddf4a1986edebec339961de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f86120081e424abc1cc26109863ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_toks_NI, k_toks_NI = next(iter(train_dl))\n",
    "c_toks_NI, k_toks_NI = c_toks_NI.squeeze(0), k_toks_NI.squeeze(0)\n",
    "c_orig_pred_NI = model.generate(input=c_toks_NI[:,:50], max_new_tokens=50, do_sample=False)\n",
    "k_orig_pred_NI = model.generate(input=k_toks_NI[:,:50], max_new_tokens=50, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4295a0e6-f7a1-4f02-9f5d-be39b813f151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d1159a05094ff0983526d8feb92eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09ca024cda14dd9b482473d074ecfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Greedy EM--- change set: 50.0 [changed 0/3], keep set: 50.0 [kept 3/3]\n",
      "---Mean NLL--- change set: 0.1491, keep set: 0.5782\n",
      "\n",
      "\n",
      "c_NI_pred: [\". I consent to the collection, use, maintenance, and disclosure of my information in accordance with the Postmedia's Privacy Policy.\\n\\nPostmedia wants to improve your reading experience as well as share the best deals and promotions from our advertisers with you\", '       \\\\setlength{\\\\oddsidemargin}{-69pt}\\n                \\\\begin{document}$$\\\\begin{aligned} \\\\mathbf', '_ISA_USAGE = YES_ERROR;\\n\\t\\t\\t\\tCLANG_WARN_EMPTY_BODY = YES;\\n\\t\\t\\t\\tCLANG_WARN_ENUM_CONVERSION = YES;\\n\\t\\t\\t']\n",
      "\n",
      "k_NI_pred: [' of Disclaimer and Privacy Policy and Terms of Use.\\n\\nPostmedia wants to improve the user experience on our site by providing it with a broad range of information and marketing information that is tailored to meet the specific needs of its users. Postmedia', '  \\\\setlength{\\\\oddsidemargin}{-69pt}\\n           \\\\begin{document}$$\\\\begin{aligned} \\\\mathbf {x}_{\\\\mathbf {i}}', 'Objc methods = {\\n\\t\\t\\t\\t\\t\"require_typeface\": \"org.eclipse.jetty.util.runtime.runtime_runtime_runtime_runtime_runtime_runtime_runtime_runtime_runtime_runtime_']\n"
     ]
    }
   ],
   "source": [
    "def model_eval(model,c_NI:torch.LongTensor=None,c_orig_pred_NI:torch.LongTensor=None,k_NI:torch.LongTensor=None,k_orig_pred_NI:torch.LongTensor=None,I_range:list=[50,100], print_pred:bool=True):\n",
    "    \"\"\"\n",
    "    evaluate the language model on individual batches of c_toks_NI and k_toks_NI\n",
    "    \"\"\"\n",
    "    ## change set\n",
    "    (c_mean_nll, c_minK_nll), (c_NI_pred, c_NI_true) = evaluation.evaluate_nll_greedy(model, c_NI, batch_size=50)\n",
    "    if c_orig_pred_NI is not None:\n",
    "        c_NI_pred = model.generate(input=c_NI[:,:50], stop_at_eos=False, max_new_tokens=50, do_sample=False)\n",
    "        c_NI_pred, c_orig_pred_NI = c_NI_pred[...,I_range[0]:I_range[1]].to(\"cpu\"), c_orig_pred_NI[...,I_range[0]:I_range[1]].to(\"cpu\")\n",
    "        c_em_N = evaluation.compute_exact_match(c_NI_pred, c_orig_pred_NI, until_wrong=False)\n",
    "    else: ## argmax greedy decoding\n",
    "        print(\"argmax greedy decoding on c_NI\")\n",
    "        c_em_N = evaluation.compute_exact_match(c_NI_pred, c_NI_true, until_wrong=True)\n",
    "\n",
    "    ## keep set\n",
    "    (k_mean_nll, k_minK_nll), (_, _) = evaluation.evaluate_nll_greedy(model, k_NI, batch_size=50)\n",
    "    k_NI_pred = model.generate(input=k_NI[:,:50], max_new_tokens=I_range[1]-I_range[0], do_sample=False)\n",
    "    k_NI_pred, k_orig_pred_NI = k_NI_pred[...,I_range[0]:I_range[1]].to(\"cpu\"), k_orig_pred_NI[...,I_range[0]:I_range[1]].to(\"cpu\")\n",
    "    if c_orig_pred_NI is not None:\n",
    "        k_em_N = evaluation.compute_exact_match(k_NI_pred, k_orig_pred_NI, until_wrong=False)\n",
    "    else:\n",
    "        k_em_N = evaluation.compute_exact_match(k_NI_pred, k_orig_pred_NI, until_wrong=True)\n",
    "\n",
    "    ## process change and keep set\n",
    "    c_mean_nll, k_mean_nll = round(c_mean_nll[...,I_range[0]:I_range[1]].mean().detach().item(),4), round(k_mean_nll[...,I_range[0]:I_range[1]].mean().detach().item(),4)\n",
    "    \n",
    "    c_changed_frac = torch.where(c_em_N == int(I_range[1]-I_range[0]), 0, 1).sum()\n",
    "    k_kept_frac = torch.where(k_em_N == int(I_range[1]-I_range[0]), 1, 0).sum() \n",
    "\n",
    "    print(f\"---Greedy EM--- change set: {c_em_N.mean().item()} [changed {c_changed_frac}/{c_em_N.shape[0]}], keep set: {k_em_N.mean().item()} [kept {k_kept_frac}/{k_em_N.shape[0]}]\")\n",
    "    print(f\"---Mean NLL--- change set: {c_mean_nll}, keep set: {k_mean_nll}\\n\\n\")\n",
    "    \n",
    "    if print_pred:\n",
    "        print(f\"c_NI_pred: {model.to_string(c_NI_pred)}\\n\")\n",
    "        print(f\"k_NI_pred: {model.to_string(k_NI_pred)}\")\n",
    "            \n",
    "model_eval(model, c_toks_NI, c_orig_pred_NI, k_toks_NI, k_orig_pred_NI, I_range=[50,100])\n",
    "#model_eval(model, c_toks_NI, None, k_toks_NI, k_orig_pred_NI, I_range=[50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd9588c5-0383-452e-9a3c-3788049d983c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_topK_grads(model, topK:float=0.001, c_types:list=[\"W_in\", \"W_out\", \"W_K\", \"W_V\", \"W_Q\", \"W_O\"]):\n",
    "    \"\"\"\n",
    "    find the topK weights in all model parameters\n",
    "    \"\"\"\n",
    "    ## (1) collect all params\n",
    "    all_param_grads = list()\n",
    "    all_param_grads = torch.cat([torch.abs(param.grad.view(-1)) for name, param in model.named_parameters() if name.split(\".\")[-1] in c_types])\n",
    "\n",
    "    ## (2) identify top params (sparsity)\n",
    "    if 0.0 < topK < 1.0: ## percentage\n",
    "        topk_vals_flat, topk_idcs_flat = torch.topk(all_param_grads, k=int(topK*len(all_param_grads)), largest=True)\n",
    "    elif 1.0 <= topK < len(all_param_grads): ## pick top weights\n",
    "        topk_vals_flat, topk_idcs_flat = torch.topk(all_param_grads, k=int(topK), largest=True)\n",
    "    min_grad = torch.min(topk_vals_flat)\n",
    "    print(f\"{len(topk_idcs_flat)} weights in {c_types} with grads > {min_grad.item()}\")\n",
    "    return min_grad, topk_idcs_flat\n",
    "\n",
    "def clip_grads(model, min_grad:float=None, full_remove_idcs:list=[]): ## in-place\n",
    "    \"\"\"\n",
    "    clip gradients than are not above min_grad, and not below min_grad if keep_neg is enabled\n",
    "    \"\"\"\n",
    "    #param_vec = torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    #torch.nn.utils.vector_to_parameters(param_vec, model.parameters())\n",
    "    print(f\"clipped at {min_grad} or using full_remove_idcs on {len(full_remove_idcs)} modules\")\n",
    "    list_idx = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            if min_grad is not None:\n",
    "                remove_idcs = torch.where((param.grad > min_grad) | (param.grad < -min_grad), 0, 1)\n",
    "            else:\n",
    "                remove_idcs = full_remove_idcs[list_idx]\n",
    "                list_idx += 1\n",
    "            param.grad[remove_idcs.bool()] = 0.0 ## annul small positive and negative grads\n",
    "            if min_grad is not None:\n",
    "                full_remove_idcs.append(remove_idcs)\n",
    "    return full_remove_idcs\n",
    "                \n",
    "                \n",
    "#min_grad, topk_idcs_flat = find_topK_grads(model, topK=0.01)\n",
    "#full_remove_idcs = clip_grads(model, min_grad)\n",
    "#full_remove_idcs = clip_grads(model, min_grad=None, full_remove_idcs=full_remove_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b30a8b5-baab-4b44-8fe9-25b47aabc8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pool_tensor(orig_tensor:torch.tensor, dims:list, match_size:tuple=None):\n",
    "    if dims is not None:\n",
    "        for dim in dims:\n",
    "            orig_tensor_pooled = orig_tensor.mean(dim)\n",
    "            if match_size is None:\n",
    "                orig_tensor = torch.repeat_interleave(orig_tensor_pooled.unsqueeze(dim), orig_tensor.shape[dim], dim=dim)\n",
    "            else: ## expand the size of pooled tensor dimension to match that of match_size\n",
    "                orig_tensor = torch.repeat_interleave(orig_tensor_pooled.unsqueeze(dim), match_size[dim], dim=dim)\n",
    "    return orig_tensor\n",
    "\n",
    "\n",
    "def contrast_metric(c_nll:torch.tensor, k_nll:torch.tensor=None, k_nll_fixed:torch.tensor=None, I_range:list=[0,100], norm_sets:float=None, pool:dict={\"c\": [-1], \"k\": [0,-1]}, only_set:str=None):\n",
    "    \"\"\"\n",
    "    minimizing / preserve keep_score while maximizing change_score\n",
    "    \"\"\"\n",
    "    if only_set is not None: ## option for making one set zero for baseline tests\n",
    "        if only_set==\"c\":\n",
    "            k_nll.detach()\n",
    "            del k_nll\n",
    "            k_nll = torch.zeros(c_nll.shape, requires_grad=False)\n",
    "        elif only_set==\"k\":\n",
    "            c_nll.detach()\n",
    "            del c_nll            \n",
    "            c_nll = torch.zeros(k_nll.shape, requires_grad=False)\n",
    "\n",
    "    ## (1) preprocess________________________________________\n",
    "    ## select latter tokens to apply metric to\n",
    "    c_nll, k_nll = c_nll[...,I_range[0]:I_range[1]], k_nll[...,I_range[0]:I_range[1]]\n",
    "        \n",
    "    ## (2) pooling_______________________________________________\n",
    "    ## pool over dims but then expand again to retain shapes\n",
    "    c_nll = pool_tensor(c_nll, pool[\"c\"], match_size=k_nll.shape) \n",
    "    k_nll = pool_tensor(k_nll, pool[\"k\"], match_size=None)\n",
    "                \n",
    "    ## adjust shapes\n",
    "    ## clip batch sizes and paragraph lengths always to shorter version\n",
    "    #c_nll = c_nll[:k_nll.shape[0], :k_nll.shape[1]]\n",
    "    #k_nll = k_nll[:c_nll.shape[0], :c_nll.shape[1]]\n",
    "    \n",
    "    #if norm_sets: ## balance out loss terms  \n",
    "    #    c_nll = torch.nn.functional.normalize(c_nll, p=1.0, dim=-1)\n",
    "    #    k_nll = torch.nn.functional.normalize(k_nll, p=1.0, dim=-1)\n",
    "    print(f\"pooling c_nll {c_nll.shape}, k_nll {k_nll.shape} pool: {pool}\")\n",
    "    \n",
    "    ## (3) apply metric_______________________________________________\n",
    "    \n",
    "    if only_set==\"c\":\n",
    "        contrast_res = c_nll.mean()\n",
    "    elif only_set==\"k\":\n",
    "        contrast_res = k_nll.mean()\n",
    "    else: ## normal case\n",
    "        if isinstance(norm_sets, float):\n",
    "            c_nll = c_nll * (norm_sets*(k_nll.detach().sum() / c_nll.detach().sum())) \n",
    "            #c_nll = c_nll * norm_sets\n",
    "        if k_nll_fixed is not None: ## mean squared error version to enforce non-changing keep set NLL\n",
    "            k_nll_fixed = k_nll_fixed[...,I_range[0]:I_range[1]]\n",
    "            #k_nll_fixed = pool_tensor(k_nll_fixed, pool[\"k\"], match_size=None)\n",
    "            #k_nll_fixed = k_nll_fixed[:k_nll.shape[0], :k_nll.shape[1]]\n",
    "            k_mse = 1000* (k_nll-k_nll_fixed.detach())**2 \n",
    "            #kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "            #contrast_res = kl_loss(c_nll, k_nll_fixed.detach())\n",
    "            contrast_res = (k_mse - c_nll).mean()\n",
    "            print(f\"contrast loss: {contrast_res}, c_nll: {c_nll.detach().mean()}, k_nll_mse: {k_mse.detach().mean()}\")\n",
    "        else:\n",
    "            contrast_res = (k_nll - c_nll).mean()\n",
    "            print(f\"contrast loss: {contrast_res}, c_nll: {c_nll.detach().mean()}, k_nll: {k_nll.detach().mean()}\")\n",
    "    return contrast_res, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87113bac-0afd-4f74-b98e-9acf7099d942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset model gpt-neo-125M\n",
      "Loaded pretrained model gpt-neo-125M into HookedTransformer\n",
      "added optimizer with lr: 1e-05 and weight_decay: 1.0\n",
      "setting no_grad on ['embed', 'pos_embed', 'unembed', 'b_in', 'b_out', 'b_K', 'b_Q', 'b_V', 'b_O']\n",
      "pooling c_nll torch.Size([3, 50]), k_nll torch.Size([3, 50]) pool: {'c': [], 'k': []}\n",
      "contrast loss: 0.4332226812839508, c_nll: 0.14667823910713196, k_nll: 0.5799009203910828\n",
      "1/5, optimizer step\n",
      "pooling c_nll torch.Size([3, 50]), k_nll torch.Size([3, 50]) pool: {'c': [], 'k': []}\n",
      "contrast loss: 0.3361014425754547, c_nll: 0.1701779067516327, k_nll: 0.5062793493270874\n",
      "2/5, optimizer step\n",
      "pooling c_nll torch.Size([3, 50]), k_nll torch.Size([3, 50]) pool: {'c': [], 'k': []}\n",
      "contrast loss: 0.2607945501804352, c_nll: 0.19398583471775055, k_nll: 0.4547803997993469\n",
      "3/5, optimizer step\n",
      "pooling c_nll torch.Size([3, 50]), k_nll torch.Size([3, 50]) pool: {'c': [], 'k': []}\n",
      "contrast loss: 0.18579351902008057, c_nll: 0.23012681305408478, k_nll: 0.41592031717300415\n",
      "4/5, optimizer step\n",
      "pooling c_nll torch.Size([3, 50]), k_nll torch.Size([3, 50]) pool: {'c': [], 'k': []}\n",
      "contrast loss: 0.10290971398353577, c_nll: 0.283720463514328, k_nll: 0.386630117893219\n",
      "5/5, optimizer step\n"
     ]
    }
   ],
   "source": [
    "def run_fwd_bwd(model, metric_fn, c_toks_NI:torch.LongTensor=None, k_toks_NI:torch.LongTensor=None, optim_steps:int=-1, topK:float=None, grad_norm:float=None, c_types:list=None):\n",
    "    \"\"\"\n",
    "    adding hooks to model, running model on data on metric and returning cached activs, params are cached in model\n",
    "    \"\"\"\n",
    "    #fwd_cache, bwd_cache = gradient.add_fwd_bwd_hooks(model, hook_filter={\"not in\":\"_input\"})     \n",
    "    c_toks_NI = c_toks_NI.to(model.cfg.device)\n",
    "    k_toks_NI = k_toks_NI.to(model.cfg.device)\n",
    "    k_nll_fixed = modelHandlers.gather_token_scores(modelHandlers.NegLogLik(model(k_toks_NI)), k_toks_NI)\n",
    "    #k_logits_fixed = torch.nn.functional.softmax(model(k_toks_NI), dim=-1)\n",
    "\n",
    "    for step_i in range(abs(optim_steps)):\n",
    "        c_nll = modelHandlers.gather_token_scores(modelHandlers.NegLogLik(model(c_toks_NI)), c_toks_NI)\n",
    "        k_nll = modelHandlers.gather_token_scores(modelHandlers.NegLogLik(model(k_toks_NI)), k_toks_NI) \n",
    "        #c_logits = torch.nn.functional.log_softmax(model(c_toks_NI), dim=-1)\n",
    "        metric_res, metric_norm = metric_fn(c_nll, k_nll)  #.to(\"cpu\")\n",
    "\n",
    "        model.zero_grad()\n",
    "        metric_res.backward(retain_graph=False)\n",
    "\n",
    "        if grad_norm is not None:\n",
    "            print(f\"applied grad norm clipping with max norm {grad_norm}\")\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_norm), norm_type=2.0)\n",
    "\n",
    "        if topK is not None:\n",
    "            if step_i == 0:\n",
    "                min_grad, topk_idcs = find_topK_grads(model, topK=topK, c_types=c_types)\n",
    "                full_remove_idcs = clip_grads(model, min_grad)\n",
    "            full_remove_idcs = clip_grads(model, min_grad=None, full_remove_idcs=full_remove_idcs)\n",
    "\n",
    "        if optim_steps >= 1 and hasattr(model, 'optim'):\n",
    "            print(f\"{step_i+1}/{abs(optim_steps)}, optimizer step\")\n",
    "            model.optim.step()\n",
    "            model.optim.zero_grad()\n",
    "        \n",
    "    del c_toks_NI\n",
    "    del k_toks_NI\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "        #fwd_cache = transformer_lens.ActivationCache(fwd_cache, model)\n",
    "        #bwd_cache = transformer_lens.ActivationCache(bwd_cache, model)\n",
    "    #return fwd_cache, bwd_cache, topk_idcs\n",
    "\n",
    "model = modelHandlers.load_model(model, lr=1e-05, weight_decay=1.0)\n",
    "metric_fn = functools.partial(contrast_metric, I_range=[49,99], pool={\"c\":[],\"k\":[]}, norm_sets=None)\n",
    "fwd_bwd_fn = functools.partial(run_fwd_bwd, optim_steps=5, topK=None, grad_norm=None, c_types=[\"W_in\",\"W_out\",\"W_K\",\"W_Q\",\"W_V\"])\n",
    "fwd_bwd_fn(model, metric_fn, c_toks_NI, k_toks_NI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7868c62e-55ca-484e-961f-19ed9c380538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71875c815184938b746e6c48b954616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9500deed9b9d4d69a862aef80de84528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Greedy EM--- change set: 19.66666603088379 [changed 2/3], keep set: 32.66666793823242 [kept 1/3]\n",
      "---Mean NLL--- change set: 0.3563, keep set: 0.3621\n",
      "\n",
      "\n",
      "c_NI_pred: ['.\\n\\nPostmedia wants to improve the user experience on our site by providing it with the latest user-friendly content, tools and functionality. This includes offering new features and premium content that add new functionality and are offered by third parties. Postmedia', '       \\\\setlength{\\\\oddsidemargin}{-69pt}\\n                \\\\begin{document}$$\\\\begin{aligned} \\\\mathbf', '_EXCEPTIONS = YES;\\n\\t\\t\\t\\tCLANG_WARN_DOCUMENTATION_COMMENTS = YES;\\n\\t\\t\\t\\tCLANG_WARN_EMPTY_BODY = YES;\\n\\t\\t\\t\\t']\n",
      "\n",
      "k_NI_pred: [' of Disclaimer and Privacy Policy and Terms of Use.\\n\\nPostmedia wants to improve the user experience on our site by providing it with the latest and greatest features by improving user experience by removing links to content that violates the Terms of Service and the', '  \\\\setlength{\\\\oddsidemargin}{-69pt}\\n           \\\\begin{document}$$\\\\begin{aligned} \\\\mathbf {x}_{\\\\mathbf {i}}', 'Objc methods = {\\n\\t\\t\\t\\t\\t\"require_typeface\": \"path\",\\n\\t\\t\\t\\t\\t\"require_typesafe_c\": \"path\",\\n\\t\\t\\t\\t\\t\"require_typesafe_auto']\n"
     ]
    }
   ],
   "source": [
    "model_eval(model, c_toks_NI, c_orig_pred_NI, k_toks_NI, k_orig_pred_NI, I_range=[50,100])\n",
    "#model_eval(model, c_toks_NI, None, k_toks_NI, k_orig_pred_NI, I_range=[50,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10857078-1968-4ebc-ac87-1dfcbfea597d",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f959e86-d378-4f3a-a399-921d76cf6806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_test_NI, k_test_NI = next(iter(test_dl))\n",
    "c_test_NI, k_test_NI = c_test_NI.squeeze(0), k_test_NI.squeeze(0)\n",
    "c_orig_test_NI = model.generate(input=c_test_NI[:,:50], max_new_tokens=50, do_sample=False)\n",
    "k_orig_test_NI = model.generate(input=k_test_NI[:,:50], max_new_tokens=50, do_sample=False)\n",
    "model_eval(model, c_test_NI, c_orig_test_NI, k_test_NI, k_orig_test_NI, I_range=[50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243f4e5-588b-4080-ac71-4007e790b4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54a0914-39d1-413a-be7e-9396353137bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-venv",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "venv (Local)",
   "language": "python",
   "name": "conda-root-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
