{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8028f7a",
   "metadata": {},
   "source": [
    "\n",
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-2qpyRvJYRR"
   },
   "source": [
    "# Single Sequence Activation Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6686,
     "status": "ok",
     "timestamp": 1702663207018,
     "user": {
      "displayName": "Niklas Stoehr",
      "userId": "03296628557932703282"
     },
     "user_tz": 480
    },
    "id": "UBSfNkEuJMGQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import transformer_lens\n",
    "import torch, gc, itertools, functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/')\n",
    "from paraMem.utils import modelHandlers, dataLoaders, gradient, localizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nn9H9zcaN4Hg"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6203,
     "status": "ok",
     "timestamp": 1702663213774,
     "user": {
      "displayName": "Niklas Stoehr",
      "userId": "03296628557932703282"
     },
     "user_tz": 480
    },
    "id": "QpqHz8RkJTix",
    "outputId": "7d76e1f4-ed39-4c65-b30f-e55d8075b9d5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"gpt-neo-125M\"\n",
    "model = modelHandlers.load_model(model_type=model_type, DEVICE=\"cpu\")\n",
    "modelHandlers.set_no_grad(model, [\"embed\", \"pos_embed\", \"unembed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYuz0oTRvDPu"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1339,
     "status": "ok",
     "timestamp": 1702663215109,
     "user": {
      "displayName": "Niklas Stoehr",
      "userId": "03296628557932703282"
     },
     "user_tz": 480
    },
    "id": "AIHeyTY1Sj3U",
    "outputId": "87916eaf-59a3-4424-92a8-00b5b73ee87c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mem and non-mem set\n",
    "#(mem_prompts, mem_counts),(non_mem_prompts,non_mem_counts) = dataLoaders.load_pile_splits(\"acc/gpt2-medium\", as_torch=True)\n",
    "#train_dl, test_dl = dataLoaders.train_test_batching(mem_prompts, non_mem_prompts, mem_batch=10, non_mem_batch=10, test_frac=0.0, shuffle=True, set_twice=\"k\")\n",
    "#c_toks_NI, k_toks_NI = next(iter(train_dl))\n",
    "#c_toks_NI, k_toks_NI = c_toks_NI.squeeze(0), k_toks_NI.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load perturbed mem set and original mem set\n",
    "mem_perturbed_sets  = dataLoaders.load_pile_splits(f\"{model_type}/perturbed\", file_names=[\"mem_toks.pt\", \"perturbed_mem_toks.pt\"], as_torch=True)\n",
    "mem_set, perturbed_mem_set = mem_perturbed_sets[0], mem_perturbed_sets[1]\n",
    "train_dl, test_dl = dataLoaders.train_test_batching(mem_set, perturbed_mem_set, mem_batch=30, non_mem_batch=30, matched=True, shuffle=False, test_frac=0.2, add_bos=None)\n",
    "c_toks_NI, c_perturb_toks_NI = next(iter(train_dl))\n",
    "c_toks_NI, c_perturb_toks_NI, = c_toks_NI.squeeze(0), c_perturb_toks_NI.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#string_NI = [\" headlines out of Washington never seem to slow. Subscribe to The D.C. Brief to make sense of what matters most. Please enter a valid email address. Sign Up Now Check the box if you do not wish to receive promotional offers via email from TIME. You can unsubscribe at any time. By signing up you are agreeing to our Terms of Use and Privacy Policy . This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Thank you! For your\"]\n",
    "string_NI = [\"Sign up for Take Action Now and get three actions in your inbox every week. You will receive occasional promotional offers for programs that support The Nationâ€™s journalism. You can read our Privacy Policy here. Sign up for Take Action Now and get three actions in your inbox every week.\\n\\nThank you for signing up. For more from The Nation, check out our latest issue\\n\\nSubscribe now for as little as $2 a month!\\n\\nSupport Progressive Journalism The Nation is reader supported:\"]\n",
    "#string_NI = [\"The following are trademarks or service marks of Major League Baseball entities and may be used only with permission of Major League Baseball Properties, Inc. or the relevant Major League Baseball entity: Major League, Major League Baseball, MLB, the silhouetted batter logo, World Series, National League, American League, Division Series, League Championship Series, All-Star Game, and the names, nicknames, logos, uniform designs, color combinations, and slogans designating the Major League Baseball clubs and entities, and\"]\n",
    "c_toks_NI = model.to_tokens(string_NI, prepend_bos=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpp4gs-7T7r_"
   },
   "source": [
    "## Gradient Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_seq_metric(nll_NI:torch.tensor, I_range:list=None, pool:dict={\"c\": []}):\n",
    "    \"\"\"\n",
    "    minimizing / preserve keep_score while maximizing change_score\n",
    "    \"\"\"\n",
    "    ## (1) preprocess________________________________________\n",
    "    ## select tokens to apply metric to\n",
    "    nll_NI = nll_NI[...,I_range[0]:I_range[1]]\n",
    "        \n",
    "    ## (2) pooling_______________________________________________\n",
    "    ## pool over dims but then expand again to retain shapes\n",
    "    nll_NI = gradient.pool_tensor(nll_NI, pool[\"c\"])             \n",
    "    print(f\"pooling nll_NI {nll_NI.shape}, pool: {pool}\")\n",
    "    \n",
    "    ## (3) apply metric_______________________________________________\n",
    "    metric_res = nll_NI.mean()\n",
    "    print(f\"contrast loss: {metric_res}\")\n",
    "    return metric_res, None\n",
    "\n",
    "metric = functools.partial(single_seq_metric, I_range=[49,50])\n",
    "c_fwd_cache, c_bwd_cache, _ = gradient.run_single_fwd_bwd(model, metric_fn=metric, c_toks_NI=c_toks_NI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "POOL_FN = {\"l1\": lambda x, dim: torch.norm(x, p=1, dim=dim),\n",
    "         \"l2\": lambda x, dim: torch.norm(x, p=2, dim=dim),\n",
    "         \"frob\": lambda x, dim: torch.linalg.matrix_norm(x, ord='fro'), ## toDo: issue requires 2D input\n",
    "         \"mean_abs\": lambda x, dim: torch.mean(torch.abs(x), dim=dim),\n",
    "         \"mean\": lambda x, dim: torch.mean(x, dim=dim),\n",
    "         \"max_abs\": lambda x, dim: torch.max(torch.abs(x), dim=dim)[0],\n",
    "         \"max\": lambda x, dim: torch.max(x, dim=dim)[0],\n",
    "         \"pass\": lambda x, dim: (x)}\n",
    "\n",
    "DIST_FN = {\"cos\": lambda x1, x2: torch.nn.functional.cosine_similarity(x1, x2, dim=-1),\n",
    "           \"sub\": lambda x1, x2: x1-x2,\n",
    "           \"sub_abs\": lambda x1, x2: torch.abs(x1-x2)}\n",
    "\n",
    "def pool_tensor(tensor:torch.tensor, pool:str=\"max\", abs_vals:bool=True, topP:float=1.0, norm_by_entries:bool=False):\n",
    "    \"\"\"\n",
    "    pool a tensor and normalize it by the number of entries\n",
    "    \"\"\"\n",
    "    n_params = tensor.numel()\n",
    "    if len(tensor.shape) == 5: ##ATTN\n",
    "        n_params = n_params / 12 ## devide by number of heads    \n",
    "\n",
    "    if abs_vals: ## take absolute values\n",
    "        tensor = torch.abs(tensor)\n",
    "        \n",
    "    norm_by = 1.0\n",
    "    if norm_by_entries:\n",
    "        norm_by = math.log(n_params)#n_params**(1/2)\n",
    "        tensor[tensor!=0] = tensor[tensor!=0]*(1/norm_by)\n",
    "    \n",
    "    if 0.0 < topP < 1.0:\n",
    "        topP = max(int(topP*tensor.shape[-1]), 1) \n",
    "    topK_vals, topK_idcs = torch.topk(tensor, int(topP), dim=-1, largest=True)  \n",
    "    tensorpool = POOL_FN[pool](topK_vals, dim=-1) ## do pooling\n",
    "    \n",
    "    #print(f\"abs_vals {abs_vals}, topP {topP} selected, {pool} pooled and normalized by: {norm_by}\")\n",
    "    return tensorpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_pool(cache:dict, second_cache:dict=None, c_type:str=None):\n",
    "    vals, names = localizing.collect_c_type(model=model, cache=cache, c_type=c_type)\n",
    "    if second_cache is not None: ## take the difference before pooling\n",
    "        vals2, names2 = localizing.collect_c_type(model=model, cache=second_cache, c_type=c_type)\n",
    "        #vals, vals2 = (vals2.sum()/vals.sum())*vals, vals2 ## normalizing\n",
    "        #vals = (vals - vals2)\n",
    "        pool_vals = torch.nn.functional.cosine_similarity(vals, vals2, dim=-1)\n",
    "    else:\n",
    "        pool_vals = pool_tensor(vals, pool=\"max\", abs_vals=True, topP=0.1, norm_by_entries=False)\n",
    "    \n",
    "    ## consider either all tokens or only selected token, then mean over sequences\n",
    "    pool_vals = pool_vals[:,:49].mean(0)\n",
    "        \n",
    "    ## reshape for plotting\n",
    "    names = list()\n",
    "    if len(pool_vals.shape) == 2: ## attention\n",
    "        names = [f\"{c_type} H{i}\" for i in range(0,pool_vals.shape[1])]\n",
    "    else: ## mlp\n",
    "        pool_vals = pool_vals.unsqueeze(-1)\n",
    "        names = [f\"{c_type}\"]\n",
    "            \n",
    "    return pool_vals, names ## I, L, H, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Activation Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1702663221595,
     "user": {
      "displayName": "Niklas Stoehr",
      "userId": "03296628557932703282"
     },
     "user_tz": 480
    },
    "id": "E7A_L7raPRCP",
    "outputId": "6ff0497f-39c0-445d-b07a-0466090f8492",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gather_activation_grads(cache:dict, second_cache:dict=None,c_types:list=[\"q\"]): #\"attn_out\", \"mlp_out\", \"z\", \"pre\", \"post\"  #\"k\", \"q\", \"v\", \"z\", \"pre\", \"post\", \"attn_out\", \"mlp_out\",  \"z\", \"mlp_in\", \"post\",\"mlp_out\"\n",
    "    vals, names = [], []\n",
    "    for c_type in c_types:\n",
    "        c_type_vals, c_type_names = collect_pool(cache, second_cache, c_type=c_type)\n",
    "        vals.append(c_type_vals)\n",
    "        names += c_type_names\n",
    "    vals = torch.cat(vals, dim=-1)\n",
    "    return vals, names\n",
    "\n",
    "c_type = \"v\"\n",
    "c_vals, names = gather_activation_grads(c_bwd_cache, c_types=[c_type])\n",
    "#diff_vals, names = gather_activation_grads({\"forward\":c_fwd_cache,\"backward\":c_bwd_cache}[fwd_bwd], {\"forward\":k_fwd_cache,\"backward\":k_bwd_cache}[fwd_bwd], idcs_NI=idcs_NI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "executionInfo": {
     "elapsed": 1634,
     "status": "ok",
     "timestamp": 1702663223223,
     "user": {
      "displayName": "Niklas Stoehr",
      "userId": "03296628557932703282"
     },
     "user_tz": 480
    },
    "id": "ZhSRhiT9Xkxm",
    "outputId": "d2a81bf0-c291-4814-c576-b41aaa201e32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "head = 2\n",
    "if len(c_vals.shape)==4:\n",
    "    vals = c_vals[:,:,head,:].squeeze().T  ## I, L, H, C\n",
    "else:\n",
    "    vals = c_vals.T\n",
    "\n",
    "fontsize = 12\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 3), gridspec_kw={'hspace': 0.4})\n",
    "s = sns.heatmap(vals,\n",
    "              cmap=mpl.colormaps[\"Reds\"], center=None,\n",
    "              xticklabels=model.to_str_tokens(c_toks_NI[:,:49]),\n",
    "              yticklabels=np.arange(0,12), square=False,\n",
    "              cbar_kws={'location': 'right','pad': 0.01})\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax.set_title(f\"Activation gradients of {c_type} H{head}\", fontsize=fontsize, loc=\"left\")\n",
    "ax.set_ylabel(\"layer\", fontsize=fontsize)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "s.set_yticklabels(s.get_yticklabels(), rotation=0, horizontalalignment='right')\n",
    "fig.savefig(f\"{dataLoaders.ROOT}/results/activ_grads_layers.pdf\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = 1\n",
    "if len(c_vals.shape)==4:\n",
    "    vals = c_vals[:,layer,:,:].squeeze().T  ## I, L, H, C\n",
    "else:\n",
    "    vals = c_vals.T\n",
    "\n",
    "fontsize = 12\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 3), gridspec_kw={'hspace': 0.4})\n",
    "s = sns.heatmap(vals,\n",
    "              cmap=mpl.colormaps[\"Reds\"], center=None,\n",
    "              xticklabels=model.to_str_tokens(c_toks_NI[:,:49]),\n",
    "              yticklabels=np.arange(0,12), square=False,\n",
    "              cbar_kws={'location': 'right','pad': 0.01})\n",
    "\n",
    "ax.set_title(f\"{c_type} activation gradients at layer {layer}\", fontsize=fontsize, loc=\"left\")\n",
    "ax.set_ylabel(\"head\", fontsize=fontsize)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "s.set_yticklabels(s.get_yticklabels(), rotation=0, horizontalalignment='right')\n",
    "fig.savefig(f\"{dataLoaders.ROOT}/results/activ_grads_heads.pdf\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOGB+zTCDBCeQ63kuW0ZbKy",
   "mount_file_id": "1gH7CY-pBqILLrkbypyW6G3qurtUUqqwl",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-venv",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "venv (Local)",
   "language": "python",
   "name": "conda-root-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
