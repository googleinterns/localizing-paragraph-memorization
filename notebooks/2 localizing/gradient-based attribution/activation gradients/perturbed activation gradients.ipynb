{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc54daa",
   "metadata": {},
   "source": [
    "\n",
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeacd3b-9676-4786-b485-66e016c9cb82",
   "metadata": {},
   "source": [
    "# Change-Perturbed Change Set Activation Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3b83b-4985-4bcb-ae2c-78cd8d023e69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import transformer_lens, torch, gc, itertools, functools, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, SymLogNorm\n",
    "\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/')\n",
    "from paraMem.utils import modelHandlers, dataLoaders, gradient, localizing, patching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439948d-afb3-472f-8dde-c0ea88b5b4df",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32953121-1ca8-40ff-96eb-63b822bdfe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"gpt-neo-125M\"\n",
    "model = modelHandlers.load_model(model_type=model_type, DEVICE=\"cpu\")\n",
    "modelHandlers.set_no_grad(model, [\"embed\", \"pos_embed\", \"unembed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30427e33-f8b3-422e-9768-9180d7f3749e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55858e0-69e2-480d-b52d-c815e35ea41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## mem and non-mem set\n",
    "#(mem_prompts, mem_counts),(non_mem_prompts,non_mem_counts) = dataLoaders.load_pile_splits(\"acc/gpt2-medium\", as_torch=True)\n",
    "#train_dl, test_dl = dataLoaders.train_test_batching(mem_prompts, non_mem_prompts, mem_batch=10, non_mem_batch=10, test_frac=0.0, shuffle=True, set_twice=\"k\")\n",
    "#c_toks_NI, k_toks_NI = next(iter(train_dl))\n",
    "#c_toks_NI, k_toks_NI = c_toks_NI.squeeze(0), k_toks_NI.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889db778-a055-4669-9ed6-d5a27ec0e311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load perturbed mem set and original mem set\n",
    "mem_perturbed_sets  = dataLoaders.load_pile_splits(f\"{model_type}/perturbed\", file_names=[\"mem_toks.pt\", \"perturbed_mem_toks.pt\"], as_torch=True)\n",
    "mem_set, perturbed_mem_set = mem_perturbed_sets[0], mem_perturbed_sets[1]\n",
    "train_dl, test_dl = dataLoaders.train_test_batching(mem_set, perturbed_mem_set, mem_batch=30, non_mem_batch=30, matched=True, shuffle=False, test_frac=0.2, add_bos=None)\n",
    "c_toks_NI, c_perturb_toks_NI = next(iter(train_dl))\n",
    "c_toks_NI, c_perturb_toks_NI, = c_toks_NI.squeeze(0), c_perturb_toks_NI.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e85f36-80f4-4a35-bbc9-ebcbe2e0be72",
   "metadata": {},
   "source": [
    "### Identify Intervention Token and Impact Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf846a-ecf4-42bf-b037-428349c7c2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_interv_impact_indeces(c_toks_NI:torch.tensor, k_toks_NI:torch.tensor):\n",
    "    \"\"\"\n",
    "    function to get the positions of the intervention (src) and impact (trg) token\n",
    "    \"\"\"\n",
    "    ck_diff_mask = torch.where(c_toks_NI != k_toks_NI, 1,0)\n",
    "    ck_diff_cumsum = torch.cumsum(ck_diff_mask, dim=-1) ## intervention\n",
    "\n",
    "    ## find intervention\n",
    "    src_NI = (ck_diff_cumsum==1).nonzero() \n",
    "    src_idcs = torch.cat((torch.zeros(1),(src_NI[:-1,0] != src_NI[1:,0]).nonzero(as_tuple=True)[0] + 1)).long()\n",
    "    #src_NI = torch.sub(src_NI, torch.cat((torch.zeros(src_NI.shape[0],1),torch.ones(src_NI.shape[0],1)), dim=-1), alpha=1) ## -1 because we care about what token is predicted\n",
    "    src_NI = src_NI[src_idcs].long() \n",
    "\n",
    "    ## find impact\n",
    "    trg_NI = (ck_diff_cumsum==2).nonzero() \n",
    "    trg_NI_idcs = torch.cat((torch.zeros(1),(trg_NI[:-1,0] != trg_NI[1:,0]).nonzero(as_tuple=True)[0] + 1)).long()\n",
    "    trg_NI = torch.sub(trg_NI, torch.cat((torch.zeros(trg_NI.shape[0],1),torch.ones(trg_NI.shape[0],1)), dim=-1), alpha=1) ## -1 because we care about what token is predicted\n",
    "    trg_NI = trg_NI[trg_NI_idcs].long() \n",
    "    \n",
    "    return src_NI, trg_NI\n",
    "\n",
    "src_NI, trg_NI = get_interv_impact_indeces(c_toks_NI, c_perturb_toks_NI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6877cf2-e733-41e3-8710-6dc3ec43726b",
   "metadata": {},
   "source": [
    "## Run backprop and collect activations step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b177f-4603-4dca-883b-fca522327799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_seq_metric(nll_NI:torch.tensor, NI_idcs:torch.tensor=None, pool:dict={\"c\": []}):\n",
    "    \"\"\"\n",
    "    minimizing / preserve keep_score while maximizing change_score\n",
    "    \"\"\"\n",
    "    ## (1) preprocess________________________________________\n",
    "    ## select tokens to apply metric to\n",
    "    nll_NI = nll_NI[NI_idcs[:,0], NI_idcs[:,1]]\n",
    "    #nll_NI = nll_NI[...,49:]\n",
    "        \n",
    "    ## (2) pooling_______________________________________________\n",
    "    ## pool over dims but then expand again to retain shapes\n",
    "    nll_NI = gradient.pool_tensor(nll_NI, pool[\"c\"])             \n",
    "    print(f\"pooling nll_NI {nll_NI.shape}, pool: {pool}\")\n",
    "    \n",
    "    ## (3) apply metric_______________________________________________\n",
    "    metric_res = nll_NI.mean()\n",
    "    print(f\"contrast loss: {metric_res}\")\n",
    "    return metric_res, None\n",
    "\n",
    "metric = functools.partial(single_seq_metric, NI_idcs=trg_NI)\n",
    "c_fwd_cache, c_bwd_cache, _ = gradient.run_single_fwd_bwd(model, metric_fn=metric, c_toks_NI=c_toks_NI)\n",
    "k_fwd_cache, k_bwd_cache, _ = gradient.run_single_fwd_bwd(model, metric_fn=metric, c_toks_NI=c_perturb_toks_NI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a59594-3c09-4cbc-af33-4d86a69d327c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "POOL_FN = {\"l1\": lambda x, dim: torch.norm(x, p=1, dim=dim),\n",
    "         \"l2\": lambda x, dim: torch.norm(x, p=2, dim=dim),\n",
    "         \"frob\": lambda x, dim: torch.linalg.matrix_norm(x, ord='fro'), ## toDo: issue requires 2D input\n",
    "         \"mean_abs\": lambda x, dim: torch.mean(torch.abs(x), dim=dim),\n",
    "         \"mean\": lambda x, dim: torch.mean(x, dim=dim),\n",
    "         \"max_abs\": lambda x, dim: torch.max(torch.abs(x), dim=dim)[0],\n",
    "         \"max\": lambda x, dim: torch.max(x, dim=dim)[0],\n",
    "         \"pass\": lambda x, dim: (x)}\n",
    "\n",
    "DIST_FN = {\"cos\": lambda x1, x2: torch.nn.functional.cosine_similarity(x1, x2, dim=-1),\n",
    "           \"sub\": lambda x1, x2: x1-x2,\n",
    "           \"sub_abs\": lambda x1, x2: torch.abs(x1-x2)}\n",
    "\n",
    "def pool_tensor(tensor:torch.tensor, pool:str=\"max\", abs_vals:bool=True, topP:float=1.0, norm_by_entries:bool=False):\n",
    "    \"\"\"\n",
    "    pool a tensor and normalize it by the number of entries\n",
    "    \"\"\"\n",
    "    n_params = tensor.numel()\n",
    "    if len(tensor.shape) == 5: ##ATTN\n",
    "        n_params = n_params / 12 ## devide by number of heads    \n",
    "\n",
    "    if abs_vals: ## take absolute values\n",
    "        tensor = torch.abs(tensor)\n",
    "        \n",
    "    norm_by = 1.0\n",
    "    if norm_by_entries:\n",
    "        norm_by = math.log(n_params)#n_params**(1/2)\n",
    "        tensor[tensor!=0] = tensor[tensor!=0]*(1/norm_by)\n",
    "    \n",
    "    if 0.0 < topP < 1.0:\n",
    "        topP = max(int(topP*tensor.shape[-1]), 1) \n",
    "    topK_vals, topK_idcs = torch.topk(tensor, int(topP), dim=-1, largest=True)  \n",
    "    tensorpool = POOL_FN[pool](topK_vals, dim=-1) ## do pooling\n",
    "    \n",
    "    #print(f\"abs_vals {abs_vals}, topP {topP} selected, {pool} pooled and normalized by: {norm_by}\")\n",
    "    return tensorpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3e352-d2d6-453d-b4d2-d1cb38ded27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_pool(cache:dict, second_cache:dict=None, c_type:str=None, idcs_NI=None):\n",
    "    vals, names = localizing.collect_c_type(model=model, cache=cache, c_type=c_type)\n",
    "    if second_cache is not None: ## take the difference before pooling\n",
    "        vals2, names2 = localizing.collect_c_type(model=model, cache=second_cache, c_type=c_type)\n",
    "        #vals, vals2 = (vals2.sum()/vals.sum())*vals, vals2 ## normalizing\n",
    "        #vals = (vals - vals2)\n",
    "        pool_vals = torch.nn.functional.cosine_similarity(vals, vals2, dim=-1)\n",
    "    else:\n",
    "        pool_vals = pool_tensor(vals, pool=\"max\", abs_vals=True, topP=0.1, norm_by_entries=False)\n",
    "    \n",
    "    ## consider either all tokens or only selected token, then mean over sequences\n",
    "    if idcs_NI is not None: \n",
    "        pool_vals = pool_vals[idcs_NI[:,0],idcs_NI[:,1]].mean(0)\n",
    "    else:\n",
    "        pool_vals = pool_vals.mean(1).mean(0)\n",
    "        \n",
    "    ## reshape for plotting\n",
    "    names = list()\n",
    "    if len(pool_vals.shape) == 2: ## attention\n",
    "        names = [f\"{c_type} H{i}\" for i in range(0,pool_vals.shape[1])]\n",
    "    else: ## mlp\n",
    "        pool_vals = pool_vals.unsqueeze(-1)\n",
    "        names = [f\"{c_type}\"]\n",
    "    return pool_vals, names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1f145-7cd0-4a8a-97b2-421ac9dd557b",
   "metadata": {},
   "source": [
    "## Activation Gradient Pooling and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eed5d3-79ce-4cc4-b2f2-bb84ea49f833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fwd_bwd = \"forward\"\n",
    "tok_pos = \"target\"\n",
    "\n",
    "idcs_NI = {\"source\":src_NI,\"target\":trg_NI}[tok_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b0ae8-dd7a-4b67-bbcd-9207cf66f450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gather_activation_grads(cache:dict, second_cache:dict=None,idcs_NI:torch.tensor=None,c_types:list=[\"k\",\"q\"]): #\"attn_out\", \"mlp_out\", \"z\", \"pre\", \"post\"  #\"k\", \"q\", \"v\", \"z\", \"pre\", \"post\", \"attn_out\", \"mlp_out\",  \"z\", \"mlp_in\", \"post\",\"mlp_out\"\n",
    "    vals, names = [], []\n",
    "    for c_type in c_types:\n",
    "        c_type_vals, c_type_names = collect_pool(cache, second_cache, c_type=c_type, idcs_NI=idcs_NI)\n",
    "        vals.append(c_type_vals)\n",
    "        names += c_type_names\n",
    "    vals = torch.cat(vals, dim=-1)\n",
    "    return vals, names\n",
    "\n",
    "c_vals, names = gather_activation_grads({\"forward\":c_fwd_cache,\"backward\":c_bwd_cache}[fwd_bwd], idcs_NI=idcs_NI)\n",
    "k_vals, names = gather_activation_grads({\"forward\":k_fwd_cache,\"backward\":k_bwd_cache}[fwd_bwd], idcs_NI=idcs_NI)\n",
    "#diff_vals, names = gather_activation_grads({\"forward\":c_fwd_cache,\"backward\":c_bwd_cache}[fwd_bwd], {\"forward\":k_fwd_cache,\"backward\":k_bwd_cache}[fwd_bwd], idcs_NI=idcs_NI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eecbda-23ee-4f45-9e82-0bdddf5f60ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fontsize = 12\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 7), gridspec_kw={'hspace': 0.4})\n",
    "\n",
    "plot_types = [\"memorized\", \"perturbed memorized\"] #\"grad differences\", \"diff after pooling\",\n",
    "#plot_types = [\"keep set 1\", \"keep set 2\", \"diff after pooling\", \"diff before pooling\"]\n",
    "\n",
    "cmaps = [\"PuOr\", \"PuOr\"] #\"coolwarm\", \"binary\"\n",
    "centering = [None, 0.0, 0.0, None]\n",
    "vals = [c_vals, k_vals]#[k_vals, c_vals]  #((k_vals.sum()/c_vals.sum())*c_vals)-k_vals \n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    plot_vals = vals[i].numpy() \n",
    "    #sns.heatmap(plot_vals[1:11,:],cmap=mpl.colormaps[cmaps[i]],center=centering[i],xticklabels=names,yticklabels=np.arange(1,plot_vals.shape[0]-1),square=False,ax=ax, cbar_kws={'location': 'right','pad': 0.01})\n",
    "    # norm=SymLogNorm(linthresh=1.0))\n",
    "    sns.heatmap(plot_vals[:,:],cmap=mpl.colormaps[cmaps[i]],center=centering[i],xticklabels=names,yticklabels=np.arange(0,plot_vals.shape[0]),square=False,ax=ax, cbar_kws={'location': 'right','pad': 0.01})# norm=SymLogNorm(linthresh=1.0))\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"{plot_types[i]}: {fwd_bwd} activations at {tok_pos} token\", fontsize=fontsize, loc=\"left\")\n",
    "    ax.set_ylabel(\"layer\")\n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize-2)\n",
    "\n",
    "    \n",
    "#fig.savefig(f\"{dataLoaders.ROOT}/results/{tok_pos}_{fwd_bwd}_{model_type}.pdf\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5039fa3-0fb8-4aad-8ecb-35f8f44d4a2c",
   "metadata": {},
   "source": [
    "## Final Plot Creation_____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b616b3-667a-46a8-9991-d481fab080c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tok_pos = \"source\"\n",
    "fwd_bwd = \"backward\"\n",
    "source_c_vals, names = gather_activation_grads({\"forward\":c_fwd_cache,\"backward\":c_bwd_cache}[fwd_bwd], idcs_NI= {\"source\":src_NI,\"target\":trg_NI}[tok_pos])\n",
    "source_k_vals, names = gather_activation_grads({\"forward\":k_fwd_cache,\"backward\":k_bwd_cache}[fwd_bwd], idcs_NI= {\"source\":src_NI,\"target\":trg_NI}[tok_pos])\n",
    "\n",
    "tok_pos = \"target\"\n",
    "fwd_bwd = \"backward\"\n",
    "target_c_vals, names = gather_activation_grads({\"forward\":c_fwd_cache,\"backward\":c_bwd_cache}[fwd_bwd], idcs_NI= {\"source\":src_NI,\"target\":trg_NI}[tok_pos])\n",
    "target_k_vals, names = gather_activation_grads({\"forward\":k_fwd_cache,\"backward\":k_bwd_cache}[fwd_bwd], idcs_NI= {\"source\":src_NI,\"target\":trg_NI}[tok_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c62ca-2117-474a-b2af-9619745622e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 11\n",
    "fig, axs = plt.subplots(2, 2, figsize=(17, 7), gridspec_kw={'hspace': 0.3,'wspace': 0.025})\n",
    "\n",
    "paragraph_types = [\"memorized\", \"perturbed memorized\"]\n",
    "token_types = [\"perturbed\", \"first impacted\"]\n",
    "vals = [[source_c_vals, target_c_vals], [source_k_vals, target_k_vals]]\n",
    "cmaps = [\"RdBu_r\", \"PuOr\"] \n",
    "centering = [0, 0]\n",
    "\n",
    "for i, ax_row in enumerate(axs):\n",
    "    for j, ax in enumerate(ax_row):\n",
    "        plot_vals = vals[i][j].numpy() \n",
    "        #s = sns.heatmap(plot_vals[1:,:],cmap=mpl.colormaps[cmaps[i]],center=centering[i],xticklabels=names,yticklabels=np.arange(1,plot_vals.shape[0]),square=False,ax=ax, cbar_kws={'location': 'right','pad': 0.01})\n",
    "        # norm=SymLogNorm(linthresh=1.0))\n",
    "        s =sns.heatmap(plot_vals[:,:],cmap=mpl.colormaps[cmaps[i]],center=centering[i],xticklabels=names,yticklabels=np.arange(0,plot_vals.shape[0]),square=False,ax=ax, cbar_kws={'location': 'right','pad': 0.01})# norm=SymLogNorm(linthresh=1.0))\n",
    "        ax.invert_yaxis()\n",
    "        if i==0:\n",
    "            ax.set_title(f\"activation gradients at {token_types[j]} token (mean over 50 paragraphs)\", fontsize=fontsize, loc=\"left\")\n",
    "        if j==0:\n",
    "            ax.set_ylabel(\"layer\")\n",
    "        ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "        s.set_yticklabels(s.get_yticklabels(), rotation=0, horizontalalignment='right')\n",
    "\n",
    "axs[0,0].set_ylabel('memorized', rotation=90, color=\"red\", fontsize=fontsize, labelpad=5)\n",
    "axs[1,0].set_ylabel('perturbed memorized', rotation=90, color=\"purple\", fontsize=fontsize, labelpad=5)\n",
    "        \n",
    "fig.savefig(f\"{dataLoaders.ROOT}/results/activ_grads_perturbed.pdf\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de844cd-aa56-4c31-84e9-2c71f66107bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-venv",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "venv (Local)",
   "language": "python",
   "name": "conda-root-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
