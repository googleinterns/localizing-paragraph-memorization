{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5252fb10",
   "metadata": {},
   "source": [
    "\n",
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpwqVC2kjN_s"
   },
   "source": [
    "# Unigram Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "error",
     "timestamp": 1702676568256,
     "user": {
      "displayName": "Niklas Stoehr",
      "userId": "03296628557932703282"
     },
     "user_tz": 480
    },
    "id": "iZh_O22ejqP_",
    "outputId": "bb9d09e3-a0b7-49aa-e3a0-f400c6e531f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformer_lens, torch, tqdm, copy, collections, operator, scipy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from toolz import compose\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/')\n",
    "from paraMem.utils import modelHandlers, dataLoaders, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"gpt-neo-125M\"\n",
    "model = modelHandlers.load_model(model_type=model_type, DEVICE=\"cpu\")\n",
    "modelHandlers.set_no_grad(model, [\"embed\", \"pos_embed\", \"unembed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mem_nonmem_sets  = dataLoaders.load_pile_splits(f\"gpt-neo-125M/preds\", file_names=[\"50_50_preds.pt\", \"0_10_preds.pt\"], as_torch=True)\n",
    "mem_set, nonmem_set = mem_nonmem_sets[0], mem_nonmem_sets[1]\n",
    "full_corpus = torch.cat((mem_set, nonmem_set), dim=0) ## attention in both the mem_set and nonmem_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_unigram_dict(model, token_set:torch.LongTensor):\n",
    "    unigram_str_dict, unigram_tok_dict = collections.Counter(),collections.Counter() \n",
    "    for n in tqdm.tqdm(range(token_set.shape[0])):\n",
    "        tok_ids = token_set[n].tolist()\n",
    "        str_toks = model.to_str_tokens(token_set[n])\n",
    "        for tok, tok_id in zip(str_toks, tok_ids):\n",
    "            unigram_str_dict[tok] += 1\n",
    "            unigram_tok_dict[tok_id] += 1\n",
    "            \n",
    "    return unigram_str_dict, unigram_tok_dict\n",
    "            \n",
    "str_counter, tok_counter = create_unigram_dict(model, full_corpus)\n",
    "mem_str_counter, mem_tok_counter = create_unigram_dict(model, mem_set)\n",
    "nonmem_str_counter, nonmem_tok_counter = create_unigram_dict(model, nonmem_set)\n",
    "\n",
    "mem_toks = set(mem_set.flatten().tolist())\n",
    "nonmem_toks = set(nonmem_set.flatten().tolist())\n",
    "\n",
    "#tok_counter_ranks = collections.Counter(dict(zip(tok_counter.keys(), rankdata(list(tok_counter.values()), method='dense')-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_filter_tokens(unigram_dict:dict, topP:float=0.1, bottomP:float=0.0):\n",
    "    n_toks = len(unigram_dict.keys())\n",
    "    top_n, bottom_n= int(n_toks*topP), n_toks-int(n_toks*bottomP)\n",
    "    remove_keys = [k for i, k in enumerate(unigram_dict.keys()) if (i <= top_n) or (i >= bottom_n)]\n",
    "    print(f\"remove_keys {len(remove_keys)} from {n_toks}\")\n",
    "    return remove_keys\n",
    "    \n",
    "topP = 0.5\n",
    "remove_keys = get_filter_tokens(tok_counter, topP=topP, bottomP=0.0)\n",
    "tok_counter_filtered = {k: v for k, v in tok_counter.items() if k not in remove_keys}\n",
    "\n",
    "remove_keys = get_filter_tokens(mem_tok_counter, topP=topP, bottomP=0.0)\n",
    "mem_tok_counter_filtered = {k: v for k, v in mem_tok_counter.items() if k not in remove_keys}\n",
    "\n",
    "remove_keys = get_filter_tokens(nonmem_tok_counter, topP=topP, bottomP=0.0)\n",
    "nonmem_tok_counter_filtered = {k: v for k, v in nonmem_tok_counter.items() if k not in remove_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mem_str_counter.subtract(nonmem_str_counter)\n",
    "#mem_tok_counter.subtract(nonmem_tok_counter)\n",
    "\n",
    "#nonmem_str_counter.subtract(mem_str_counter)\n",
    "#nonmem_tok_counter.subtract(mem_tok_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Attention in all heads in Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_attention_scores(model, toks_NI:torch.LongTensor, layer:int=1, tok_idx:int=50, n_limit:int=100):\n",
    "    \n",
    "    n_heads = 12\n",
    "    attn_head_dicts = [collections.defaultdict(float) for head_idx in range(0,n_heads)]\n",
    "    attn_head_entropy_N = []\n",
    "    \n",
    "    for i, toks_I in tqdm.tqdm(enumerate(toks_NI[:n_limit,:])):\n",
    "        _, activs = model.run_with_cache(toks_I.unsqueeze(0).to(model.cfg.device))\n",
    "        \n",
    "        activs = activs.to(\"cpu\")\n",
    "        attn_pattern = activs[\"pattern\", layer, \"attn\"].squeeze()\n",
    "        lookback_HI = attn_pattern[:,tok_idx,:tok_idx]\n",
    "        \n",
    "        ## collect entropy of attention-token distribution\n",
    "        entropy_H = scipy.stats.entropy(lookback_HI, axis=-1)\n",
    "        attn_head_entropy_N.append(entropy_H)\n",
    "        \n",
    "        for prefix_idx in range(lookback_HI.shape[-1]):\n",
    "            for head_idx in range(len(attn_head_dicts)):\n",
    "                attn_head_dicts[head_idx][toks_I[prefix_idx].item()] += lookback_HI[head_idx,prefix_idx].item() \n",
    "    \n",
    "    attn_head_entropy_NH = np.stack(attn_head_entropy_N)\n",
    "    attn_head_entropy_H = attn_head_entropy_NH.mean(0)\n",
    "    return attn_head_dicts, attn_head_entropy_H\n",
    "            \n",
    "\n",
    "full_corpus = full_corpus[torch.randperm(full_corpus.size()[0])]\n",
    "attn_head_dicts, attn_head_entropy_H = collect_attention_scores(model, full_corpus, n_limit=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#attn_head_dicts_meaned = []\n",
    "#for attn_head_dict in attn_head_dicts:\n",
    "#    attention_sum = np.array(list(attn_head_dict.values())).sum()\n",
    "#    attn_head_dict_meaned = dict(zip(list(attn_head_dict.keys()), list(attn_head_dict.values()) / attention_sum))\n",
    "#    attn_head_dicts_meaned.append(attn_head_dict_meaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_correlations(freq_dict:dict, attn_dict:dict, filter_tok_list:list=None, title:str=\"\"):\n",
    "    \n",
    "    if filter_tok_list is None:\n",
    "        filter_tok_list = list(freq_dict.keys())\n",
    "    \n",
    "    tok_freqs, attn_scores = [], []\n",
    "    for attn_tok_id, attn_score in attn_dict.items():\n",
    "        if attn_tok_id in freq_dict.keys() and attn_tok_id in filter_tok_list:\n",
    "            tok_freq = freq_dict[attn_tok_id]\n",
    "            tok_freqs.append(tok_freq)\n",
    "            attn_scores.append(attn_score)\n",
    "            \n",
    "    tok_freqs, attn_scores = np.array(tok_freqs), np.array(attn_scores)\n",
    "    #coef, p = scipy.stats.spearmanr(np.array(tok_freqs), np.array(attn_scores))       \n",
    "    coef, p = scipy.stats.kendalltau(tok_freqs, attn_scores)\n",
    "    entropy = scipy.stats.entropy(attn_scores)\n",
    "    #coef, p = scipy.stats.pearsonr(np.array(tok_freqs), np.array(attn_scores))\n",
    "    print(f\"{title}---n tokens: {len(tok_freqs)}, correlation: {round(coef,3)}, p: {round(p,3)}, entropy: {round(entropy,2)}\")\n",
    "    return coef, entropy, tok_freqs, attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Correlations per Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_corrs, mem_corrs, nonmem_corrs = [], [], []\n",
    "full_entropys, mem_entropys, nonmem_entropys = [], [], []\n",
    "all_head_tok_freqs, all_head_attn_scores = [], []\n",
    "\n",
    "for head_idx in range(len(attn_head_dicts)):  \n",
    "    print(f\"\\n{head_idx}____________________\")\n",
    "    full_corr,full_entropy,tok_freqs,attn_scores = get_correlations(tok_counter_ranks, attn_head_dicts[head_idx], filter_tok_list=None, title=\"full corpus toks\")\n",
    "    mem_corr,mem_entropy,_,_ = get_correlations(mem_tok_counter_filtered, attn_head_dicts[head_idx], filter_tok_list=None, title=\"memorized toks\")\n",
    "    nonmem_corr,nonmem_entropy,_,_ = get_correlations(nonmem_tok_counter_filtered, attn_head_dicts[head_idx], filter_tok_list=None, title=\"non-memorized toks\")\n",
    "    \n",
    "    full_corrs.append(full_corr)\n",
    "    mem_corrs.append(mem_corr)\n",
    "    nonmem_corrs.append(nonmem_corr)\n",
    "    \n",
    "    full_entropys.append(full_entropy)\n",
    "    mem_entropys.append(mem_entropy)\n",
    "    nonmem_entropys.append(nonmem_entropy)\n",
    "    \n",
    "    all_head_tok_freqs.append(tok_freqs)\n",
    "    all_head_attn_scores.append(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(int(len(all_head_tok_freqs)/4), 4, figsize=(10, 5), constrained_layout=True) #gridspec_kw={'hspace': 0.2}\n",
    "#fontsize = 12\n",
    "\n",
    "#for i, ax in enumerate(axs.flatten()):  \n",
    "#    ax.set_title(f\"L1 H{i}, Attn Entropy {round(full_entropys[i],2)}\", fontsize=fontsize, loc='left')\n",
    "#    x,y = np.array(all_head_tok_freqs[i]), np.array(all_head_attn_scores[i]) \n",
    "#    em = ax.scatter(x, y, color=\"grey\", alpha=0.2, label=\"exact match\")\n",
    "#    ax.set_xscale('log')\n",
    "#    ax.set_yscale('log')\n",
    "    #ax.set_xlim(0, 10000)\n",
    "    #ax.set_ylim(None, 1)\n",
    "    \n",
    "    #b, a = np.polyfit(x, y, deg=1)\n",
    "    #x_seq = np.arange(0,x.max())\n",
    "    #ax.plot(x_seq, a + b * x_seq, color=\"black\", lw=1.5, linestyle=\":\");\n",
    "#    ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    \n",
    "#fig.supxlabel('Token Frequency', fontsize=fontsize)\n",
    "#fig.supylabel('Average Attention at Token', fontsize=fontsize)#, x=0.085)\n",
    "#fig.savefig(f\"{dataLoaders.ROOT}/results/attn_entropy.pdf\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 2.5), gridspec_kw={'hspace': 0.4})\n",
    "fontsize=12\n",
    "\n",
    "x = np.arange(len(mem_corrs))  # the label locations\n",
    "width, offset = 0.25, 0.25  # the width of the bars\n",
    "\n",
    "label_full = ax.bar(x + -offset, full_corrs, width, color=\"grey\", label=\"full corpus\")\n",
    "label_mem = ax.bar(x, mem_corrs, width, color=\"red\", label=\"memorized paragraphs\")\n",
    "label_nonmem = ax.bar(x + +offset, nonmem_corrs, width , color=\"blue\", label=\"non-memorized paragraphs\")\n",
    "\n",
    "ax.set_ylabel(\"correlation coefficient\",fontsize=fontsize)\n",
    "ax.set_xlabel(\"Layer 1, Head X\",x=0.85,fontsize=fontsize)\n",
    "\n",
    "ax.axhline(y=0.0, c=\"black\",linewidth=1.5,zorder=10)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "ax.text(0.01, -0.17, f'all tokens in corpus', color=\"grey\", fontsize=fontsize, horizontalalignment='left',verticalalignment='top', transform=ax.transAxes)\n",
    "ax.text(0.18, -0.17, f'tokens in memorized paragraphs', color=\"red\", fontsize=fontsize, horizontalalignment='left',verticalalignment='top', transform=ax.transAxes)\n",
    "ax.text(0.45, -0.17, f'tokens in non-memorized paragraphs', color=\"blue\", fontsize=fontsize, horizontalalignment='left',verticalalignment='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_title(f\"Kendall correlation between attention at tokens and the tokens' frequencies\", fontsize=fontsize, loc=\"left\")\n",
    "loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "fig.savefig(f\"{dataLoaders.ROOT}/results/attn_freq_kendall.pdf\", dpi=200, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2.5), gridspec_kw={'hspace': 0.4})\n",
    "fontsize=10\n",
    "\n",
    "x = np.arange(len(mem_corrs))  # the label locations\n",
    "width, offset = 0.8, 0.25  # the width of the bars\n",
    "\n",
    "label_mem = ax.bar(x, full_corrs, width, color=\"grey\", label=\"memorized paragraphs\")\n",
    "\n",
    "ax.set_ylabel(\"Kendall corr. coeff.\",fontsize=fontsize)\n",
    "ax.set_xlabel(\"Layer 1, Head X\",fontsize=fontsize)\n",
    "ax.set_ylim(np.array(full_corrs).min()-0.02, None)\n",
    "\n",
    "ax.axhline(y=0.0, c=\"black\",linewidth=1.5,zorder=10)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "\n",
    "ax.set_title(f\"Correlation between attention\\nat tokens and the tokens' frequencies\", fontsize=fontsize, loc=\"left\")\n",
    "loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "fig.savefig(f\"{dataLoaders.ROOT}/results/attn_freq_kendall.pdf\", dpi=200, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO+ZxywYIB/Bk/v8xqveO03",
   "gpuType": "T4",
   "mount_file_id": "169QQ_PVjQB_juISfLhE9Vuevo3PkQsNF",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-venv",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "venv (Local)",
   "language": "python",
   "name": "conda-root-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
