{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c662a5",
   "metadata": {},
   "source": [
    "\n",
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpwqVC2kjN_s"
   },
   "source": [
    "# Rare Token Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "error",
     "timestamp": 1702676568256,
     "user": {
      "displayName": "Niklas Stoehr",
      "userId": "03296628557932703282"
     },
     "user_tz": 480
    },
    "id": "iZh_O22ejqP_",
    "outputId": "bb9d09e3-a0b7-49aa-e3a0-f400c6e531f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformer_lens, torch, tqdm, copy, collections, operator\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from toolz import compose\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jupyter/')\n",
    "from paraMem.utils import modelHandlers, dataLoaders, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"gpt-neo-125M\"\n",
    "model = modelHandlers.load_model(model_type=model_type, DEVICE=\"cpu\")\n",
    "modelHandlers.set_no_grad(model, [\"embed\", \"pos_embed\", \"unembed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mem_nonmem_sets  = dataLoaders.load_pile_splits(f\"gpt-neo-125M/preds\", file_names=[\"50_50_preds.pt\", \"0_10_preds.pt\"], as_torch=True)\n",
    "mem_set, nonmem_set = mem_nonmem_sets[0], mem_nonmem_sets[1]\n",
    "full_corpus = torch.cat((mem_set, nonmem_set), dim=0) ## attention in both the mem_set and nonmem_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nonmem_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_unigram_dict(model, token_set:torch.LongTensor):\n",
    "    unigram_str_dict, unigram_tok_dict = collections.Counter(),collections.Counter() \n",
    "    for n in tqdm.tqdm(range(token_set.shape[0])):\n",
    "        tok_ids = token_set[n].tolist()\n",
    "        str_toks = model.to_str_tokens(token_set[n])\n",
    "        for tok, tok_id in zip(str_toks, tok_ids):\n",
    "            unigram_str_dict[tok] += 1\n",
    "            unigram_tok_dict[tok_id] += 1\n",
    "    return unigram_str_dict, unigram_tok_dict\n",
    "            \n",
    "full_str_counter, full_tok_counter = create_unigram_dict(model, full_corpus)\n",
    "#mem_str_counter, mem_tok_counter = create_unigram_dict(model, mem_set)\n",
    "#nonmem_str_counter, nonmem_tok_counter = create_unigram_dict(model, nonmem_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Attention per frequency of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_attn_frequency_scores(model, toks_NI:torch.LongTensor, tok_counter:dict, n_limit:int=100):\n",
    "    \n",
    "    layer, n_heads, tok_idx = 1, 12, 50\n",
    "    topK_rarest = 50\n",
    "    head_freq_attn = torch.zeros(n_heads, topK_rarest)\n",
    "    \n",
    "    for i, toks_I in tqdm.tqdm(enumerate(toks_NI[:n_limit,:])):\n",
    "        toks_I_freq = [tok_counter[tok.item()] for tok in toks_I[:tok_idx]] ## get corpus frequencies for all tokens in the paragraph\n",
    "        toks_I_freq_idcs = rankdata(torch.LongTensor(toks_I_freq), method='dense')-1 ## get frequency ranks for tokens including ties\n",
    "        \n",
    "        _, activs = model.run_with_cache(toks_I.unsqueeze(0).to(model.cfg.device))  \n",
    "        activs = activs.to(\"cpu\")\n",
    "        \n",
    "        attn_pattern = activs[\"pattern\", layer, \"attn\"].squeeze()\n",
    "        lookback_HI = attn_pattern[:,tok_idx,:tok_idx] ## collect \"lookback\" attention at token \"tok_idx\"\n",
    "        lookback_HI_idcs = torch.argsort(lookback_HI, dim=1, descending=True) ## optional: get ranks of attention scores for all toks per paragraph\n",
    "        \n",
    "        ## looping through all prefix tokens and all heads\n",
    "        for prefix_idx in range(lookback_HI.shape[-1]):\n",
    "            for head_idx in range(n_heads):\n",
    "                freq_rank = toks_I_freq_idcs[prefix_idx].item() ## get the tokens frequency rank in the sequence\n",
    "                if freq_rank < topK_rarest: ## collect only tokens with ranks below \"topK_rarest\"\n",
    "                    head_freq_attn[head_idx,freq_rank] += lookback_HI[head_idx,prefix_idx].item() ## sum attention of the token\n",
    "    return head_freq_attn\n",
    "            \n",
    "\n",
    "full_corpus=full_corpus[torch.randperm(full_corpus.size()[0]),:]\n",
    "head_freq_attn = collect_attn_frequency_scores(model, full_corpus, full_tok_counter, n_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 3.5))\n",
    "fontsize=15\n",
    "\n",
    "#norm_by = head_freq_attn.sum(1).unsqueeze(1).repeat(1, head_freq_attn.shape[1])\n",
    "#vals = (head_freq_attn / norm_by).numpy()\n",
    "vals = head_freq_attn.numpy()\n",
    "vals = vals[:,:]\n",
    "\n",
    "s = sns.heatmap(vals, #annot=rankdata(vals, axis=-1)-1\n",
    "              cmap=mpl.colormaps[\"Greys\"], center=None,\n",
    "              xticklabels=np.arange(0, vals.shape[1]),\n",
    "              yticklabels=np.arange(0, vals.shape[0]), square=False, cbar=True,\n",
    "              cbar_kws={'location': 'right', 'pad': 0.01,'label':'attention on tokens\\nover 1000 paragraphs'}, ax=ax) \n",
    "\n",
    "sns.set(font_scale=1.12)\n",
    "#ax.set_xlabel(\"10 least frequent tokens per paragraph\",fontsize=fontsize)\n",
    "ax.set_xlabel(\"tokens per paragraph ordered by corpus frequency\",fontsize=fontsize)\n",
    "\n",
    "ax.set_ylabel(\"layer 1, head X\",fontsize=fontsize)\n",
    "ax.set_title(f\"KQ attention on 'distinctive' tokens\", fontsize=fontsize, loc=\"left\")\n",
    "s.set_yticklabels(s.get_yticklabels(), rotation=0, horizontalalignment='right')\n",
    "\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "#ax2 = ax.twinx()\n",
    "#ax2.grid(False)\n",
    "#ax2.set_yticks(np.linspace(0.5, (vals.shape[0]-0.5), num=12))\n",
    "\n",
    "#kendall_corr = []\n",
    "#for head_attn in vals:\n",
    "#    coef, p = scipy.stats.kendalltau(np.arange(vals.shape[1]), head_attn)\n",
    "#    kendall_corr.append(round(coef,2))\n",
    "\n",
    "#ax2.set_yticklabels([corr for corr in kendall_corr], verticalalignment=\"center\", fontsize = fontsize-1)\n",
    "#ax2.set_ylabel(\"kendall corr. between attention\\nat token and token frequency\", fontsize = fontsize-2)\n",
    "#ax2.invert_yaxis()\n",
    "#for y_tick_pos in ax.get_yticks():\n",
    "#    ax.text(1.01, y_tick_pos, f'1', color=\"black\", fontsize=fontsize-1, horizontalalignment='left',verticalalignment='bottom', transform=ax.transAxes)\n",
    "\n",
    "fig.savefig(f\"{dataLoaders.ROOT}/results/least_frequent_50.pdf\", dpi=200, bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO+ZxywYIB/Bk/v8xqveO03",
   "gpuType": "T4",
   "mount_file_id": "169QQ_PVjQB_juISfLhE9Vuevo3PkQsNF",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-venv",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "venv (Local)",
   "language": "python",
   "name": "conda-root-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
